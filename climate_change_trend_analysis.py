# -*- coding: utf-8 -*-
"""climate change trend analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yjVph8tnPagfHqwzlRiG8rwni-OS3CbI

**Load Dataset**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv('/content/drive/My Drive/climate dataset/GlobalTemperatures.csv')

"""**Overview of Dataset**"""

df.describe()

# Get unique city names
unique_cities = df['City'].unique()
print(unique_cities)

"""**Missing Values**"""

import pandas as pd

# Checking for missing values
missing_data = df.isnull().sum()
missing_percentage = (missing_data / len(df)) * 100
print(missing_percentage)

import matplotlib.pyplot as plt

# Calculate the number of missing values in each column
missing_values = df.isnull().sum()

missing_values[missing_values > 0].plot(kind='bar', figsize=(10, 6))
plt.title('Missing Values Count per Column')
plt.ylabel('Number of Missing Values')
plt.xlabel('Columns')
plt.show()

df['dt'] = pd.to_datetime(df['dt'])

df.set_index('dt', inplace=True)

plt.figure(figsize=(12, 6))
plt.plot(df.isnull().sum(axis=1), 'r.')
plt.title('Missing Values Over Time')
plt.ylabel('Number of Missing Values')
plt.xlabel('Date')
plt.show()

missing_values = df.isnull().sum()

total_missing = missing_values.sum()

missing_percentage = (missing_values / len(df)) * 100

missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})
missing_data = missing_data[missing_data['Missing Values'] > 0].sort_values(by='Percentage', ascending=False)
print(missing_data)

num_rows = df.shape[0]
print("Total number of rows:", num_rows)

print("First Row:")
for column, value in df.iloc[0].items():
  print(f"{column}: {value}")

print("\nLast Row:")
for column, value in df.iloc[-1].items():
  print(f"{column}: {value}")

import pandas as pd

# Define two dates in the format 'YYYY-MM-DD'
start = df.iloc[0]['dt']
end = df.iloc[-1]['dt']

print(start)
print(end)

start_date = pd.to_datetime(start)
end_date = pd.to_datetime(end)

duration = end_date - start_date

# Output the duration in days
print(f"{duration.days} days")

"""*97123 is 266 years*


---


*266 years is 3192 month*


---


*apparently there is no missing date in this time period*
"""

df['dt'] = pd.to_datetime(df['dt'])
df['Month'] = df['dt'].dt.month
grouped_by_month = df.groupby('Month')

monthly_data = {}

for month, data in grouped_by_month:
    monthly_data[month] = data

january_data = monthly_data[0]
print(january_data.head())

# Save each month's data to a separate CSV
for month, data in monthly_data.items():
    data.to_csv(f'month_{month}.csv', index=False)

january_missing = january_data.isnull().sum()
print('January missing value: ')
print(january_missing)

"""**Handle Missing Values**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.linear_model import BayesianRidge

df.set_index('dt', inplace=True)

class LoggingIterativeImputer(IterativeImputer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.imputation_sequence_ = []
        self.iteration_count_ = 0

    def _impute_one_round(self, X_filled, mask_missing_values):
        """Override to add logging after each round."""
        X_filled, predictor_indices, estimator_indices = super()._impute_one_round(
            X_filled, mask_missing_values
        )
        self.iteration_count_ += 1
        print(f"Completed iteration {self.iteration_count_}")
        return X_filled, predictor_indices, estimator_indices

columns_to_impute = [
    'LandAverageTemperature',
    'LandMaxTemperature',
    'LandMinTemperature',
    'LandAndOceanAverageTemperature',
    'LandAverageTemperatureUncertainty',
    'LandMaxTemperatureUncertainty',
    'LandMinTemperatureUncertainty',
    'LandAndOceanAverageTemperature'
]

imputer = LoggingIterativeImputer(
    estimator=BayesianRidge(),
    max_iter=10,
    random_state=0,
    initial_strategy='mean'
)

data_to_impute = df[columns_to_impute]

# Fit the imputer and transform the data
imputed_array = imputer.fit_transform(data_to_impute)

df_imputed = pd.DataFrame(imputed_array, columns=columns_to_impute, index=data_to_impute.index)

for column in columns_to_impute:
    plt.figure(figsize=(12, 6))
    sns.kdeplot(df[column], label='Original', color='blue')
    sns.kdeplot(df_imputed[column], label='Imputed', color='red', linestyle='--')
    plt.title(f'Distribution of {column} Before and After Imputation')
    plt.legend()
    plt.show()

corr_original = df[columns_to_impute].corr()

corr_imputed = df_imputed.corr()


fig, axes = plt.subplots(1, 2, figsize=(15, 6))
sns.heatmap(corr_original, annot=True, cmap='coolwarm', ax=axes[0])
axes[0].set_title('Original Data Correlation')
sns.heatmap(corr_imputed, annot=True, cmap='coolwarm', ax=axes[1])
axes[1].set_title('Imputed Data Correlation')
plt.show()

n_imputations = 5
imputed_datasets = []

for i in range(n_imputations):
    imputer = LoggingIterativeImputer(
        estimator=BayesianRidge(),
        max_iter=10,
        random_state=i,
        initial_strategy='mean'
    )
    imputed_array = imputer.fit_transform(data_to_impute)
    df_imputed = pd.DataFrame(imputed_array, columns=columns_to_impute, index=data_to_impute.index)
    imputed_datasets.append(df_imputed)

stacked_data = np.stack([df.values for df in imputed_datasets])

# Compute mean and standard deviation
mean_imputations = np.mean(stacked_data, axis=0)
std_imputations = np.std(stacked_data, axis=0)

mean_df = pd.DataFrame(mean_imputations, columns=columns_to_impute, index=df.index)
std_df = pd.DataFrame(std_imputations, columns=columns_to_impute, index=df.index)

missing_counts_imputed = df_imputed.isnull().sum()
missing_counts_imputed = missing_counts_imputed[missing_counts_imputed > 0]

if missing_counts_imputed.empty:
    print("No missing values remain after imputation.")
else:
    missing_counts_imputed.plot.bar()
    plt.title('Number of Missing Values per Column After Imputation')
    plt.ylabel('Count of Missing Values')
    plt.show()

# Save to CSV
df_imputed.to_csv('imputed_climate_data.csv')

